
# ğŸ›¡ï¸ GhewarkeGeeks â€“ LLM Safety Browser Extension

A **browser extension** that adds a safety layer while interacting with **Large Language Models (LLMs)** on websites like ChatGPT, Gemini, Claude, etc.
It detects unsafe, harmful, or restricted content in real time and alerts the user before submission or response usage.

---

## ğŸ¯ Problem Statement

LLMs can sometimes generate or accept:

* Harmful instructions
* Unsafe prompts
* Policy-violating content
* Toxic or sensitive text

Users often submit prompts without realizing the risks.
This extension helps **prevent unsafe interactions** by analyzing text **locally** before it is sent or used.

---

## ğŸš€ Features

* ğŸ” **Real-Time Text Scanning**

  * Monitors text typed into prompt boxes on supported websites.

* ğŸ§  **AI-Based Content Classification**

  * Uses a trained ML model instead of simple keyword matching.

* âš ï¸ **Instant Warnings**

  * Highlights unsafe content and notifies the user immediately.

* ğŸ”’ **Privacy-Preserving**

  * No API calls
  * No data sent to external servers
  * Everything runs locally

* âš™ï¸ **Custom Rule Support**

  * Safety rules can be extended or modified.

---

## ğŸ§© Project Structure

```
ghewarkegeeks/
â”‚
â”œâ”€â”€ model/              # Trained ML model files
â”œâ”€â”€ tokenizer/          # Tokenizer used for text preprocessing
â”œâ”€â”€ content.js          # Injected script to capture user input
â”œâ”€â”€ background.js       # Background logic and message handling
â”œâ”€â”€ scanner.js          # Core scanning and classification logic
â”œâ”€â”€ popup.html          # Extension UI
â”œâ”€â”€ popup.js            # UI logic
â”œâ”€â”€ style.css           # Styling for popup and alerts
â”œâ”€â”€ rules.json          # Safety rules and categories
â””â”€â”€ manifest.json       # Browser extension configuration
```
---

### ğŸ”„ How they work together

1. User types text on a webpage
2. `content.js` captures the input
3. Text is passed to the **tokenizer**
4. Tokenized input goes to the **model**
5. Model predicts risk category
6. Extension blocks, warns, or allows the input

---

## ğŸ› ï¸ Installation

### Step 1: Clone the repository

```bash
git clone https://github.com/priyanshu015211/ghewarkegeeks.git
cd ghewarkegeeks
```

### Step 2: Load extension in browser

* Open Chrome / Edge / Brave
* Go to `chrome://extensions`
* Enable **Developer Mode**
* Click **Load Unpacked**
* Select the project folder

---

## ğŸŒ Supported Platforms

* Google Chrome
* Microsoft Edge
* Brave Browser
* Firefox (with minor adjustments)

---

## ğŸ§ª Development & Contribution

You can contribute by:

* Improving detection accuracy
* Adding new safety categories
* Enhancing UI/UX
* Optimizing performance

Steps:

1. Fork the repository
2. Create a new branch
3. Make changes
4. Submit a Pull Request

---

## ğŸ“Œ Use Cases

* Safer AI usage for students
* Preventing accidental policy violations
* Ethical AI experimentation
* Hackathons and research demos

---

## âš–ï¸ License

This project is **open source**.
Feel free to use, modify, and extend responsibly.

---
